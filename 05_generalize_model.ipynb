{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **05. Generalize Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic**: Generalize your Model by adjusting the validation method, preprocessing, feature engineering, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(5000)\n",
    "x = np.random.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cut our Dataset into usages for training, testing and validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的架構(多少層,多深), 這些參數稱為 **Hyperparameter** (超參數)\n",
    "那像是神經網路的權重參數, 這些參數稱為 **Weight parameter** (權重參數)\n",
    "\n",
    "我們都知道 Weight parameter 會隨我們訓練來調整, 那 Hyperparameter 怎麼辦呢?\n",
    "這時就是我們 validation data 的功用了, 我們可以看 validation 的結果來調整我們的模型架構\n",
    "但這樣調整也怕過多, 造成模型去擬合我們的 validation data, 這種現象稱為 **information leak** (資訊洩漏)\n",
    "當然我們也不想用 test data 來調整我們的 hyperparameter, 讓我們的 model 去學習 test data 就失去意義了\n",
    "\n",
    "\n",
    "這邊提供幾種 validation data 驗證方法\n",
    "* Simple hold-out validation\n",
    "* K-fold validation\n",
    "* Iterated K-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple hold-out validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這種方法適合在你有很多資料的時候來使用, 概念很簡單,\n",
    "\n",
    "我們就乖乖的切 3 種 data, 分別是 train, test, validation, \n",
    "\n",
    "而 val data 通常是從 train 切出去的, 切記 val 不能太少, 不然不具有統計意義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val_data = 1000\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "val_data = data[:num_val_data]\n",
    "train_data = data[num_val_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 val data 調整完我們的參數後, 會再把 val 和 train 做合併再一起訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([train_data, val_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **K-fold validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://scikit-learn.org/stable/_images/grid_search_cross_validation.png' alt='by scikitlearn'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這方法適用於資料少的情況\n",
    "\n",
    "將我們的 train data 切成 K 折, 並以 K-1 折作為我們的 train data, 並以其中 1 折來作為我們的 val data,\n",
    "\n",
    "而這會重複 K 次來進行, 那每次選的 val data 會輪流來當, 最後我們把這些 K 次的 validation scores 來取平均作為我們的參考依據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_data = len(data)//k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "val_scores = []\n",
    "for fold in range(k):\n",
    "    val_data = data[ num_val_data*fold : num_val_data*(fold+1) ]\n",
    "    train_data = np.concatenate([data[ :num_val_data*fold ], data[ num_val_data*(fold+1): ] ])\n",
    "    \n",
    "    # 用 model evaluate 來算出 val scores 加到我們的 list\n",
    "    score = 88\n",
    "    val_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Iterated K-fold validation with shuffling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此方法適用於資料相對較少, 且需要盡可能精確的評估模型的情況\n",
    "\n",
    "這是目前在 Kaggle 常使用的方法, 簡單來講就是進行很多次的 K fold, 而每次資料都必須進行 shuffle, \n",
    "\n",
    "假設進行 P 次, 那麼就是說進行 P*K 回的訓練, 這方法相對來說運算成本相當的高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validation 注意事項**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **資料代表性**\n",
    "    * 假設我們要判斷 0～9 的資料, 我們不能把 label 0~7 為訓練, 8～9 為val, 這樣訓練就不具代表性, 因此我們需要 **randomly shuffle** 來處理資料\n",
    "* **時間方向性**\n",
    "    * 當處理時間序的資料, 我們界不能隨機取了, 因為這樣會造成 temporal leak (時間漏失) 發生時間錯位的狀況, 因此我們要確保 test data 都在 train data 之後\n",
    "* **資料重複現象**\n",
    "    * 有時候同樣的 label 及 feature 會出現在 train 及 val data 兩次, 這樣會造成表現不可信的狀況產生, 我們要避免"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing & Feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們所用的 input 及 label 都必須將它轉換成 float 的張量來做運算, 這步驟就稱為 **Data vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex: 像是處理圖片的資料時, 我們必須將所有的值 / 255, 這是因為值得分佈範圍是 0～255, 我們透過 Normalization 的方式將範圍定在 0～1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這麼做是為了避免特徵彼此之間的影響會過大, 導致神經網路難以學習, 因此我們在做 Normalization 時, 記得每項特徵的數值範圍都應大致相同<br>\n",
    "像是我們也常定義 Normalization 的分佈為:\n",
    "* mean = 0\n",
    "* std = 1\n",
    "\n",
    "那我們可以進行以下操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Missing Value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料中有 null出現的時候, 通常在 NN 中, 遇到缺失值我們會直接補 0, 前提是 0 比須不曾被定義於原本的資料中(不具參考意義）\n",
    "\n",
    "這樣在訓練的時候 NN 就會去學習遇到 0 這個特徵時, 就知道他是缺失值,會自動去忽略它 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Hint:**\n",
    "如果說今天test data 裡有缺失值, 而 train data 沒有, 那我們就要為 train data 多加幾筆 missing value 出來<br>\n",
    "不然你的 model 根本不知道遇到 missing value 該怎麼做<br>\n",
    "簡單來做就是複製一些正常的樣本, 然後刪除幾的其中幾個特徵這樣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徵工程是透過我們自己的知識來為 input data 做 preprocessing, 這讓我們的 NN 能夠更有效率地來學習<br>\n",
    "舉例來說,今天我們想解決的問題是: 用時鐘的照片來判斷幾點幾分<br>\n",
    "如果我們直接將 img 丟進model來學習, 這會是相當沒效率又號效能的,<br>\n",
    "但如果我們今天以時鐘中心為原點, 我們分別找出時針及分針的座標,接下來我們再來計算時針及分針所產生的夾角,<br>\n",
    "我們把 img 轉成 夾角 這個動作就是所謂的特徵工程,<br>\n",
    "這樣不僅節省我們的資料空間 (input是夾角的數值,不再是一個 image)<br>\n",
    "也可以更有效率地去訓練我們的 model (資料少, 但訊息更明確)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overfitting & Underfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning 基本上就是在 Optimization 及 Generalization 之間的拉鋸戰,<br>\n",
    "Optimization 指在 train data 的表現 ; Generalization 指在未知data上的表現<br>\n",
    "在訓練我們的 model 時, 一開始 train 和 validation data 間的表現會相似,<br>\n",
    "因為 model 還沒有學習過多的 train data 特徵,<br>\n",
    "而經過一段時間後, train data 會越來越好, 而 validation 的資料表現就漸漸趨於平緩,甚至反彈,<br>\n",
    "這是因為我們的 model 學習過多的 train data 模式,就連一些微小的雜七雜八特徵都學進去了, <br>\n",
    "這樣讓未知的資料集可能不適用這個 model,因為這 model 後來變成單純 for train data 的感覺,<br>\n",
    "這過程稱之為 **Overfitting**<br>\n",
    "\n",
    "--- \n",
    "想解決這個問題, 我們可以用兩種方式:\n",
    "* Find More data ---> 避免以管窺天,當井底之蛙\n",
    "* 調整 model 的複雜程度(more easy), 限制儲存資訊量  ---> 讓他不學那麼多特徵, 不學雜七雜八, 就更 generalize\n",
    "\n",
    "像這一類避免 overfitting 的方式稱之為 **Regularization**\n",
    "\n",
    "---\n",
    "而 **Underfitting** 就是指 training 的 loss 無法降低，預測的準確率很低<br>\n",
    "這通常發生在你的 NN 架構太簡單了, Model 無法去學習<br>\n",
    "因此我們的資訊量找到一個平衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**以下介紹幾種方法來改善 Overfitting**\n",
    "* 縮減神經網路大小\n",
    "* Weight Regularization\n",
    "* Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 準備好我們的 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **縮減神經網路大小**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Origin Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "original_model = models.Sequential()\n",
    "original_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))  #原始的為 16 個單元\n",
    "original_model.add(layers.Dense(16, activation='relu'))\n",
    "original_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "original_model.compile(optimizer='rmsprop',\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Smaller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_model = models.Sequential()\n",
    "smaller_model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))  #改成容量較低的 4 個單元\n",
    "smaller_model.add(layers.Dense(4, activation='relu'))\n",
    "smaller_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "smaller_model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTnvguDD6lLj"
   },
   "outputs": [],
   "source": [
    "bigger_model = models.Sequential()\n",
    "bigger_model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))  #改以更高容量的 512 個輸出單位\n",
    "bigger_model.add(layers.Dense(512, activation='relu'))\n",
    "bigger_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "bigger_model.compile(optimizer='rmsprop',\n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 8s 316us/step - loss: 0.4523 - acc: 0.8201 - val_loss: 0.3374 - val_acc: 0.8769\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.2530 - acc: 0.9102 - val_loss: 0.2877 - val_acc: 0.8864\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.1977 - acc: 0.9286 - val_loss: 0.2811 - val_acc: 0.8890\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.1657 - acc: 0.9416 - val_loss: 0.2961 - val_acc: 0.8826\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.1470 - acc: 0.9469 - val_loss: 0.3125 - val_acc: 0.8802\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 5s 186us/step - loss: 0.1275 - acc: 0.9558 - val_loss: 0.3383 - val_acc: 0.8749\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.1142 - acc: 0.9598 - val_loss: 0.3577 - val_acc: 0.8734\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.0985 - acc: 0.9680 - val_loss: 0.3917 - val_acc: 0.8671\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0899 - acc: 0.9694 - val_loss: 0.4151 - val_acc: 0.8678\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 5s 183us/step - loss: 0.0792 - acc: 0.9737 - val_loss: 0.4462 - val_acc: 0.8639\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0695 - acc: 0.9768 - val_loss: 0.4750 - val_acc: 0.8623\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0644 - acc: 0.9784 - val_loss: 0.5008 - val_acc: 0.8602\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 5s 188us/step - loss: 0.0549 - acc: 0.9830 - val_loss: 0.5357 - val_acc: 0.8586\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0485 - acc: 0.9855 - val_loss: 0.5675 - val_acc: 0.8562\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0435 - acc: 0.9874 - val_loss: 0.6253 - val_acc: 0.8520\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0381 - acc: 0.9883 - val_loss: 0.6426 - val_acc: 0.8535\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 5s 184us/step - loss: 0.0303 - acc: 0.9919 - val_loss: 0.6830 - val_acc: 0.8508\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 5s 189us/step - loss: 0.0293 - acc: 0.9914 - val_loss: 0.7326 - val_acc: 0.8492\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 5s 187us/step - loss: 0.0231 - acc: 0.9942 - val_loss: 0.7604 - val_acc: 0.8500\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 5s 185us/step - loss: 0.0211 - acc: 0.9946 - val_loss: 0.8062 - val_acc: 0.8490\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 0.5804 - acc: 0.7027 - val_loss: 0.5309 - val_acc: 0.7558\n",
      "Epoch 2/20\n",
      " - 5s - loss: 0.4843 - acc: 0.8455 - val_loss: 0.4876 - val_acc: 0.8280\n",
      "Epoch 3/20\n",
      " - 5s - loss: 0.4389 - acc: 0.8920 - val_loss: 0.4663 - val_acc: 0.8497\n",
      "Epoch 4/20\n",
      " - 5s - loss: 0.4077 - acc: 0.9154 - val_loss: 0.4526 - val_acc: 0.8663\n",
      "Epoch 5/20\n",
      " - 5s - loss: 0.3832 - acc: 0.9306 - val_loss: 0.4513 - val_acc: 0.8620\n",
      "Epoch 6/20\n",
      " - 5s - loss: 0.3620 - acc: 0.9415 - val_loss: 0.4424 - val_acc: 0.8728\n",
      "Epoch 7/20\n",
      " - 5s - loss: 0.3425 - acc: 0.9515 - val_loss: 0.4489 - val_acc: 0.8671\n",
      "Epoch 8/20\n",
      " - 5s - loss: 0.3261 - acc: 0.9560 - val_loss: 0.4515 - val_acc: 0.8675\n",
      "Epoch 9/20\n",
      " - 5s - loss: 0.3099 - acc: 0.9613 - val_loss: 0.4687 - val_acc: 0.8626\n",
      "Epoch 10/20\n",
      " - 5s - loss: 0.2961 - acc: 0.9649 - val_loss: 0.4632 - val_acc: 0.8649\n",
      "Epoch 11/20\n",
      " - 5s - loss: 0.2815 - acc: 0.9692 - val_loss: 0.4840 - val_acc: 0.8606\n",
      "Epoch 12/20\n",
      " - 5s - loss: 0.2684 - acc: 0.9718 - val_loss: 0.4873 - val_acc: 0.8626\n",
      "Epoch 13/20\n",
      " - 5s - loss: 0.2562 - acc: 0.9753 - val_loss: 0.5214 - val_acc: 0.8574\n",
      "Epoch 14/20\n",
      " - 5s - loss: 0.2454 - acc: 0.9760 - val_loss: 0.5507 - val_acc: 0.8540\n",
      "Epoch 15/20\n",
      " - 5s - loss: 0.2340 - acc: 0.9782 - val_loss: 0.5378 - val_acc: 0.8586\n",
      "Epoch 16/20\n",
      " - 5s - loss: 0.2235 - acc: 0.9802 - val_loss: 0.5300 - val_acc: 0.8582\n",
      "Epoch 17/20\n",
      " - 5s - loss: 0.2149 - acc: 0.9811 - val_loss: 0.5766 - val_acc: 0.8550\n",
      "Epoch 18/20\n",
      " - 5s - loss: 0.2052 - acc: 0.9822 - val_loss: 0.5430 - val_acc: 0.8571\n",
      "Epoch 19/20\n",
      " - 5s - loss: 0.1965 - acc: 0.9830 - val_loss: 0.6403 - val_acc: 0.8494\n",
      "Epoch 20/20\n",
      " - 5s - loss: 0.1888 - acc: 0.9837 - val_loss: 0.7073 - val_acc: 0.8467\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 0.4614 - acc: 0.8022 - val_loss: 0.3212 - val_acc: 0.8592\n",
      "Epoch 2/20\n",
      " - 6s - loss: 0.2197 - acc: 0.9113 - val_loss: 0.2976 - val_acc: 0.8750\n",
      "Epoch 3/20\n",
      " - 6s - loss: 0.1200 - acc: 0.9560 - val_loss: 0.3318 - val_acc: 0.8689\n",
      "Epoch 4/20\n",
      " - 6s - loss: 0.0769 - acc: 0.9810 - val_loss: 0.4459 - val_acc: 0.8792\n",
      "Epoch 5/20\n",
      " - 6s - loss: 0.0660 - acc: 0.9873 - val_loss: 0.4490 - val_acc: 0.8829\n",
      "Epoch 6/20\n",
      " - 6s - loss: 0.0565 - acc: 0.9892 - val_loss: 0.4924 - val_acc: 0.8790\n",
      "Epoch 7/20\n",
      " - 6s - loss: 0.0017 - acc: 0.9999 - val_loss: 0.7924 - val_acc: 0.8660\n",
      "Epoch 8/20\n",
      " - 6s - loss: 0.0585 - acc: 0.9902 - val_loss: 0.6504 - val_acc: 0.8795\n",
      "Epoch 9/20\n",
      " - 6s - loss: 0.0694 - acc: 0.9916 - val_loss: 0.6084 - val_acc: 0.8746\n",
      "Epoch 10/20\n",
      " - 6s - loss: 3.5444e-04 - acc: 1.0000 - val_loss: 0.7234 - val_acc: 0.8785\n",
      "Epoch 11/20\n",
      " - 7s - loss: 4.4602e-05 - acc: 1.0000 - val_loss: 0.8289 - val_acc: 0.8774\n",
      "Epoch 12/20\n",
      " - 6s - loss: 0.1112 - acc: 0.9903 - val_loss: 0.8292 - val_acc: 0.8768\n",
      "Epoch 13/20\n",
      " - 7s - loss: 1.3076e-05 - acc: 1.0000 - val_loss: 0.8439 - val_acc: 0.8760\n",
      "Epoch 14/20\n",
      " - 6s - loss: 5.2692e-06 - acc: 1.0000 - val_loss: 0.8714 - val_acc: 0.8772\n",
      "Epoch 15/20\n",
      " - 6s - loss: 2.2800e-06 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.8781\n",
      "Epoch 16/20\n",
      " - 6s - loss: 6.6195e-07 - acc: 1.0000 - val_loss: 0.9915 - val_acc: 0.8786\n",
      "Epoch 17/20\n",
      " - 6s - loss: 2.1403e-07 - acc: 1.0000 - val_loss: 1.0390 - val_acc: 0.8784\n",
      "Epoch 18/20\n",
      " - 6s - loss: 1.3502e-07 - acc: 1.0000 - val_loss: 1.0691 - val_acc: 0.8786\n",
      "Epoch 19/20\n",
      " - 6s - loss: 1.1977e-07 - acc: 1.0000 - val_loss: 1.0837 - val_acc: 0.8783\n",
      "Epoch 20/20\n",
      " - 6s - loss: 1.1518e-07 - acc: 1.0000 - val_loss: 1.0932 - val_acc: 0.8783\n"
     ]
    }
   ],
   "source": [
    "original_hist = original_model.fit(x_train, y_train,\n",
    "                                   epochs=20,\n",
    "                                   batch_size=512,\n",
    "                                   verbose=1,\n",
    "                                   validation_data=(x_test, y_test))\n",
    "\n",
    "smaller_model_hist = smaller_model.fit(x_train, y_train,\n",
    "                                       epochs=20,\n",
    "                                       batch_size=512,\n",
    "                                       verbose=2,\n",
    "                                       validation_data=(x_test, y_test))\n",
    "\n",
    "bigger_model_hist = bigger_model.fit(x_train, y_train,\n",
    "                                     epochs=20,\n",
    "                                     batch_size=512,\n",
    "                                     verbose=2,\n",
    "                                     validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLdF6cYn2cbF"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "original_val_loss = original_hist.history['val_loss']\n",
    "smaller_model_val_loss = smaller_model_hist.history['val_loss']\n",
    "bigger_model_val_loss = bigger_model_hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "ECwoTpWq2hRs",
    "outputId": "4e78a072-c3c6-4e68-90e8-7d6c12c2268d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs, original_val_loss, 'b-', label='Original model')\n",
    "plt.plot(epochs, bigger_model_val_loss, 'r-', label='Bigger model')\n",
    "plt.plot(epochs, smaller_model_val_loss, 'g-', label='Smaller model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Weight Regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做法上是在原本的 loss function 中加上 Cost 項目, 讓較大的權重值在 Cost 上表現更明顯,<br>\n",
    "這樣最後在調整參數時就優先處理他們, 抑制他們的表現,<br> \n",
    "這樣我們就是以較小的權重值來調整我們的 model,<br>\n",
    "換句話說, 以較簡單的方式(less entropy)來解決我們的問題(調整我們的 model),這樣方式所適用的範圍會比複雜的方式更通用\n",
    "\n",
    "----------------------\n",
    "\n",
    "兩種常用方式來進行 Regularization:\n",
    "* L1: 和 abs(weight parameters) 成正比\n",
    "* L2: 和 pow(weight parameters, 2) 成正比\n",
    "\n",
    "看以下紅框框就好, 前面 loss function 會變"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/2546/1*zMLv7EHYtjfr94JOBzjqTA.png' alt='l1_l2'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# 這邊 l 是給予 lamda 的值\n",
    "L1 = regularizers.l1(l=0.0001)         # l1\n",
    "L2 = regularizers.l2(l=0.0001)         # l2\n",
    "L1_2 = regularizers.l1_l2(l1=0.01,l2=0.0001)    # l1 + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用\n",
    "\n",
    "l2_model = models.Sequential()\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),  #加入 L2 權重常規化並將學習率設為 0.001 \n",
    "                          activation='relu', input_shape=(10000,)))\n",
    "l2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                          activation='relu'))\n",
    "l2_model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout 是在 train 的時候, 隨機把某幾個數值的輸出歸 0, 而在 test 的時候就不會"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這樣做就像是給 layer 加入一些**雜訊**, 有了這些雜訊就能避免神經網路去死背 data 的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpt_model = models.Sequential()\n",
    "dpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(16, activation='relu'))\n",
    "dpt_model.add(layers.Dropout(0.5))\n",
    "dpt_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "dpt_model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
